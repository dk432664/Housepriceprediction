{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Happysad.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk432664/Housepriceprediction/blob/master/Happysad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26chntu8AydH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "c316d78e-1a31-4ec6-ac7e-890abff7df60"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\" \\\n",
        "    -O \"/tmp/happy-or-sad.zip\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-21 17:07:33--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2670333 (2.5M) [application/zip]\n",
            "Saving to: ‘/tmp/happy-or-sad.zip’\n",
            "\n",
            "\r/tmp/happy-or-sad.z   0%[                    ]       0  --.-KB/s               \r/tmp/happy-or-sad.z 100%[===================>]   2.55M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-08-21 17:07:34 (166 MB/s) - ‘/tmp/happy-or-sad.zip’ saved [2670333/2670333]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj96BoY2T9DT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzs0oCK3A028",
        "colab_type": "text"
      },
      "source": [
        "# TO create a image classifier for a set of happy or sad Images using convolution,maxpooling2D layer and by doing dataproccessing using Imagedatagenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xscEA41KBifI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile\n",
        "DESIRED_ACCURACY = 0.999\n",
        "zip_ref = zipfile.ZipFile(\"/tmp/happy-or-sad.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/h-or-s\")\n",
        "zip_ref.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkMNO6orB4l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mycallback(tf.keras.callbacks.Callback):\n",
        "  def end_on_epoch(self,epoch,logs={}):\n",
        "    if(logs.get('acc')>DESIRED_ACCURACY):\n",
        "\n",
        "      print('accuracy has been reached')\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = mycallback()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D34YbV7LKzQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=tf.keras.models.Sequential([tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(300,300,3)),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.Flatten(),\n",
        "                                  tf.keras.layers.Dense(1024,activation='relu'),\n",
        "                                  tf.keras.layers.Dense(1,activation='sigmoid')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOzLG_iaKbhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "bbd017b2-3f6d-439c-ff58-2c8f77271b9b"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(optimizer=RMSprop(lr=0.001),loss='binary_crossentropy',metrics=['acc'])\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_data=ImageDataGenerator(rescale=1/255)\n",
        "train_gen=train_data.flow_from_directory(\"/tmp/h-or-s\",  \n",
        "        target_size=(300,300), \n",
        "        batch_size=10,\n",
        "        class_mode='binary')\n",
        "hist=model.fit_generator(train_gen,\n",
        "      steps_per_epoch=2,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      callbacks=[callbacks])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0821 17:16:40.427752 139881395042176 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 80 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "2/2 [==============================] - 3s 1s/step - loss: 14.0103 - acc: 0.5500\n",
            "Epoch 2/15\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.7444 - acc: 0.5500\n",
            "Epoch 3/15\n",
            "2/2 [==============================] - 2s 950ms/step - loss: 0.5824 - acc: 0.6000\n",
            "Epoch 4/15\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.6804 - acc: 0.7000\n",
            "Epoch 5/15\n",
            "2/2 [==============================] - 2s 932ms/step - loss: 0.8614 - acc: 0.6000\n",
            "Epoch 6/15\n",
            "2/2 [==============================] - 2s 931ms/step - loss: 0.8727 - acc: 0.3500\n",
            "Epoch 7/15\n",
            "2/2 [==============================] - 2s 938ms/step - loss: 0.5552 - acc: 0.9500\n",
            "Epoch 8/15\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.8962 - acc: 0.7500\n",
            "Epoch 9/15\n",
            "2/2 [==============================] - 2s 945ms/step - loss: 0.6432 - acc: 0.6000\n",
            "Epoch 10/15\n",
            "2/2 [==============================] - 2s 941ms/step - loss: 0.3791 - acc: 0.7500\n",
            "Epoch 11/15\n",
            "2/2 [==============================] - 2s 933ms/step - loss: 0.3407 - acc: 0.9000\n",
            "Epoch 12/15\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.3892 - acc: 0.8500\n",
            "Epoch 13/15\n",
            "2/2 [==============================] - 2s 937ms/step - loss: 0.2393 - acc: 0.9500\n",
            "Epoch 14/15\n",
            "2/2 [==============================] - 2s 934ms/step - loss: 0.0749 - acc: 1.0000\n",
            "Epoch 15/15\n",
            "2/2 [==============================] - 2s 949ms/step - loss: 0.4590 - acc: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "981WpBSqKW15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}